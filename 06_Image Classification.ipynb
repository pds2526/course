{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM69NAo8z18X",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/pds2526/course/blob/main/06_Image%20Classification.ipynb\n",
    "\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmEJyQGAz18Y",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div class='bar_title'></div>\n",
    "\n",
    "*Practical Data Science*\n",
    "\n",
    "# Deep Learning for Computer Vision\n",
    "\n",
    "Gunther Gust<br>\n",
    "Chair for Enterprise AI<br>\n",
    "Data Driven Decisions Group (D3)<br>\n",
    "Center for Artificial Intelligence and Data Science (CAIDAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52JnVZo3z18Y",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/d3.png?raw=true\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ts8oq_Wbz18Y",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/CAIDASlogo.png?raw=true\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIgASMCH4g6e",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "__Credits for this lecture__\n",
    "\n",
    "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/09/oreillybook.jpg?raw=true\" width=\"500\" align=\"right\"/>\n",
    "\n",
    "**Jeremy Howard and Sylvian Gugger: \"Deep Learning for Coders with Fastai and PyTorch: AI Applications without a PhD.\" (2020).**\n",
    "\n",
    "Available as [Jupyter Notebook](https://github.com/fastai/fastbook)\n",
    "\n",
    "Materials taken from\n",
    "- https://github.com/fastai/fastbook/blob/master/05_pet_breeds.ipynb\n",
    "- https://github.com/hiromis/notes/blob/master/Lesson1.md\n",
    "- https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNtRTLxq4g6o",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Processing Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ir5Io9tRz18Y",
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "### Requirements: Graphics Processing Unit (GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ld9hU0rq4g6o",
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "\n",
    "GPU is fit for training the deep learning systems in a long run for very large datasets. CPU can train a deep learning model quite slowly. GPU accelerates the training of the model. Hence, GPU is a better choice to train the Deep Learning Model efficiently and effectively ([see here for a nice informal explanation](https://medium.com/@shachishah.ce/do-we-really-need-gpu-for-deep-learning-47042c02efe2)) (open in incognito tab to circument paywall).\n",
    "\n",
    "Make sure your GPU environment is set up and you can run Jupyter Notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### GPU on Colab\n",
    "\n",
    "* Select 'Runtime' -> 'Change runtime type' -> 'Python 3' and a 'GPU' or (Tensor Processing Unit, 'TPU') before running the notebook.\n",
    "\n",
    "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/gpu_colab.png?raw=true\" width=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EL0p7qe4g6p",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Libraries\n",
    "\n",
    "We are going to work with the fastai V2 library which sits on top of Pytorch.\n",
    "The fastai library as a [layered API](https://raw.githubusercontent.com/GuntherGust/tds2_data/refs/heads/main/images/09/fastai-api.webp) as summarized by this graph:\n",
    "\n",
    "<img src=\"https://docs.fast.ai/images/layered.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGSYTr4Sz18Z",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### FastAI Architecture\n",
    "\n",
    "#### Applications Layer\n",
    "This is the topmost layer, offering the highest level of abstraction. It provides predefined applications for different types of data such as:\n",
    "- `Vision` for image data\n",
    "- `Text` for natural language processing\n",
    "- `Tabular` for structured data like CSV files\n",
    "- `Collab` which refers to collaborative filtering used in recommendation systems\n",
    "\n",
    "#### High-Level API\n",
    "Below the applications layer, there's the high-level API, which offers more control while still being relatively abstract. This includes:\n",
    "- `Learner`: an interface for creating and training machine learning models\n",
    "- `DataBlock`: an abstraction for defining how to load, split, and prepare data for modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Mid-Level API\n",
    "This layer provides more granular control over the training process, with components like:\n",
    "- `Callbacks`: hooks for executing custom code at certain points in the training loop\n",
    "- `Generic Optimizer`: for customizing the optimization algorithm\n",
    "- `General Metric`: for customizing evaluation metrics\n",
    "- `Data Core`: the fundamental data structures upon which the higher-level abstractions are built.\n",
    "\n",
    "#### Low-Level API\n",
    "The foundation that provides maximum flexibility and control, consisting of:\n",
    "- `Pipeline`: a sequence of data processing operations\n",
    "- `Reversible Transforms`: data transformations that can be applied and then reversed\n",
    "- `OO Tensors`: object-oriented design for tensor operations\n",
    "- `Optimized ops`: highly efficient, low-level operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wgkXVAkz18Z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On google colab, you may need to install or upgrade fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxUwshbf4g6s",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#we need the current git version\n",
    "#%pip install --upgrade fastai\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dV3NvNEZz18Z",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "and import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzTqdS-Yz18Z",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZ-q11uu4g6z",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Dataset: Imagewoof\n",
    "\n",
    "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/09/dogs.jpg?raw=true\" width=\"500\" align=\"right\"/>\n",
    "\n",
    "\n",
    "We are going to use the [Imagewoof](https://github.com/fastai/imagenette) data set, a subset of 10 classes from Imagenet that aren't so easy to classify, since they're all dog breeds.\n",
    "\n",
    "The breeds are: Australian terrier, Border terrier, Samoyed, Beagle, Shih-Tzu, English foxhound, Rhodesian ridgeback, Dingo, Golden retriever, Old English sheepdog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjFiRmsR4g60",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Download and extract__\n",
    "\n",
    "The first thing we have to do is download and extract the data that we want. `untar_data` will download that to some convenient path and untar it for us and it will then return the value of path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "uuV7irXR4g61",
    "outputId": "4e6be51b-ca50-4d7c-c97c-93db1420c7d3",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# URLs.IMAGEWOOF_160 = 'https://s3.amazonaws.com/fast-ai-imageclas/imagewoof-160'\n",
    "path = untar_data(URLs.IMAGEWOOF_160)\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRPi1lwb4g64",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Next time you run this, since you've already downloaded it, it __won't download it again.__ Since you've already untared it, it won't untar it again. So everything is designed to be pretty __automatic and easy.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1c-iEEw4g65",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Looking at the data\n",
    "\n",
    "The first thing we do when we approach a problem is to take a look at the data. We always need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huhhUnzR4g66",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "__Python 3 pathlib__\n",
    "\n",
    "For convenience, fast.ai adds functionality into existing Python stuff. One of these things is add a `ls()` method to path,  an IPython magic wrapper for the shell’s `ls`, showing the filesystem __contents__ of the current __directory.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQfrkYyT4g66",
    "outputId": "7205c102-5549-4d73-b6d6-2f017e1bac78",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Lists the contents of the directory represented by 'path'\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lk17vEwrz18a",
    "outputId": "97fad017-a93a-49c6-9ee8-f72cd8a28a7f",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Lists the contents of the 'train' subdirectory in the directory represented by 'path'\n",
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mentimeter\n",
    "\n",
    "> How is the downloaded data set of dog images structured? Please answer [on Mentimeter](https://www.menti.com/aleeov3mdgtg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUq7SzoZ4g69",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "__Path objects__ from the [pathlib](https://docs.python.org/3/library/pathlib.html) module are much better to use than strings. It doesn't matter if you're on Windows, Linux, or Mac. It is always going to work exactly the __same way.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j44eTNB24g7G",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__get_image_files__\n",
    "\n",
    "`get_image_files` will just grab an array of all of the image files based on extension in a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Alu06Dr4g7H",
    "outputId": "01abafc1-df98-4e40-ae87-d910cec53c92",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fnames = get_image_files(path/'train')\n",
    "fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Niy9jFxS4g7M",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Here, 'n02115641' refers to the class _dingo_ in [imagenet](https://gist.github.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "2ZH02qpHz18a",
    "outputId": "4b37fad5-7696-44f7-d7ae-d59adbe26509",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import imageio.v3 as imageio\n",
    "# Displays the first image from the 'fnames'\n",
    "plt.imshow(imageio.imread(fnames[10]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5p5HsAc4g7M",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating DataLoaders\n",
    "\n",
    "The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the __names of the (sub-)folders.__ We will need to extract them to be able to classify the images into the correct categories.\n",
    "\n",
    "We will now explore different ways load such datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b1SDgpqz18a",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Factory Methods: ImageDataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwuTQ71vz18a",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The [`ImageDataLoaders`](https://docs.fast.ai/vision.data.html#ImageDataLoaders) Class is a basic wrapper around several DataLoaders with factory methods for computer vision problems.\n",
    "\n",
    "Here, we can use `ImageDataLoaders.from_folder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CmvAp97z18a",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Creates image data loaders from folders, specifying 'train' and 'val' for training and validation sets, and applies resizing to 224x224 pixels for each item.\n",
    "dls = ImageDataLoaders.from_folder(path, train='train', valid='val', item_tfms=Resize(224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPF_aN5Z4g7i",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__data.show_batch__\n",
    "\n",
    "Let's take a look at a few pictures. `dls.show_batch` can be used to show me some of the contents  So you can see roughly what's happened is that they all seem to have being zoomed and cropped in a reasonably nice way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "RLncxVO3z18a",
    "outputId": "23548537-16fb-464b-b5fc-4c2d77849772",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hf_KX8E44g7X",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The data block API\n",
    "\n",
    "The [data block API](https://docs.fast.ai/data.block.html#DataBlock) lets you customize the creation of the Dataloaders by isolating the underlying parts of that process in separate blocks, mainly:\n",
    "\n",
    "1. The types of your input and labels\n",
    "2. `get_items` (how to get your input)\n",
    "3. `splitter` (How to split the data into a training and validation sets?)\n",
    "4. `get_y` (How to label the inputs?)\n",
    "\n",
    "... and suitable transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61jW-5dcz18a",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "woof = DataBlock(\n",
    "    # Specifies the types of blocks: ImageBlock for the input images and CategoryBlock for the target labels (categories).\n",
    "    blocks = (ImageBlock, CategoryBlock),\n",
    "    # Sets the function to gather image files from directories\n",
    "    get_items=get_image_files,\n",
    "    # Uses GrandparentSplitter to split the dataset into training and validation sets based on folder names ('train' and 'val').\n",
    "    splitter=GrandparentSplitter(train_name='train', valid_name='val'),\n",
    "    # Sets the function to label the data items based on the parent folder name.\n",
    "    get_y=parent_label,\n",
    "    # Applies an item-wise transformation to resize each image to 460 pixels on the smallest side.\n",
    "    item_tfms=Resize(460),\n",
    "    # Applies batch-wise transformations for data augmentation, resizing images to 256 pixels and using a minimum scale of 0.75.\n",
    "    batch_tfms=aug_transforms(size=256, min_scale=0.75)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXavmPfuz18a",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "One important piece of this `DataBlock` call that we haven't seen before is in these two lines:\n",
    "\n",
    "```python\n",
    "item_tfms=Resize(460),\n",
    "batch_tfms=aug_transforms(size=224, min_scale=0.75)\n",
    "```\n",
    "\n",
    "These lines implement a fastai data augmentation strategy which we call __*presizing*.__ Presizing is a particular way to do image augmentation that is designed to minimize data destruction while maintaining good performance. When an image is randomly cropped as part of augmentation, the crop's size is a random value greater than or equal to min_scale times the size of the original image. For instance, if min_scale is set to 0.75, the random crop will be __at least 75%__ of the original image's size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yColBD2pz18a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Presizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T5pZCiHz18a",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img alt=\"Presizing on the training set\" width=\"600\" caption=\"Presizing on the training set\" id=\"presizing\" src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/09/fastai-cropping.png?raw=true\" align=\"right\">\n",
    "\n",
    "Presizing adopts two strategies\n",
    "\n",
    "1. *Crop full width or height*: This is in `item_tfms`, so it's applied to each individual image before it is copied to the GPU. It's used to ensure all images are the same size. On the training set, the crop area is chosen randomly. On the validation set, the center square of the image is always chosen.\n",
    "2. *Random crop and augment*: This is in `batch_tfms`, so it's applied to a batch all at once on the GPU, which means it's fast. On the validation set, only the resize to the final size needed for the model is done here. On the training set, the random crop and any other augmentations are done first.\n",
    "\n",
    "To implement this process in fastai you use `Resize` as an item transform with a large size, and `RandomResizedCrop` as a batch transform with a smaller size. `RandomResizedCrop` will be added for you if you include the `min_scale` parameter in your `aug_transforms` function, as was done in the `DataBlock` call in the previous section. Alternatively, you can use `pad` or `squish` instead of `crop` (the default) for the initial `Resize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "The **difference between Crop, Pad, and Squish** in the Resize transform in FastAI lies in how the image is adjusted to fit the specified size:\n",
    "\n",
    "1. **Crop (Default)**:   The image is resized while maintaining its aspect ratio, and then a central or random crop is taken to match the target size.  \n",
    "   - **Effect**: Parts of the image outside the crop area are discarded.  \n",
    "   - **Use Case**: When keeping the aspect ratio matters, and you don't mind losing some image content.\n",
    "\n",
    "2. **Pad**:   The image is resized while maintaining its aspect ratio, then padding is added to match the target size.  \n",
    "   - **Effect**: No parts of the image are lost, but padding (usually black) is added, potentially introducing unwanted artifacts.  \n",
    "   - **Use Case**: When keeping the entire image without distortion is important.\n",
    "\n",
    "3. **Squish**:  The image is resized to fit the target size without maintaining the aspect ratio.  \n",
    "   - **Effect**: The image is stretched or compressed, possibly distorting its appearance.  \n",
    "   - **Use Case**: When retaining the full image content is critical, and distortion is acceptable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNZvsHIBz18a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmZ8BqQ34g7Y",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "From the Datablock we can automatically get a our DataLoaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_F1BGdNRz18d",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dls = woof.dataloaders(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtAFMnrQz18d",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    ".. and have a look at the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C44ldUSpz18d",
    "outputId": "0100b800-3236-466a-e439-3d66361c176d",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "woof.summary(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "FMUCsR3cz18d",
    "outputId": "299563c5-5d00-4d98-ca1e-d121ce4314ff",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4NbvibI4g7Y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To make the classes easier to read and interpret, we can modify our `get_y` function. First, we define a dictionary for the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6g82_t0Y4g7Z",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dogs_dict = { 'n02086240': 'Shih-Tzu',\n",
    "              'n02087394': 'Rhodesian_ridgeback',\n",
    "              'n02088364': 'beagle',\n",
    "              'n02089973': 'English_foxhound',\n",
    "              'n02093754': 'Border_terrier',\n",
    "              'n02096294': 'Australian_terrier',\n",
    "              'n02099601': 'golden_retriever',\n",
    "              'n02105641': 'Old_English_sheepdog',\n",
    "              'n02111889': 'Samoyed',\n",
    "              'n02115641': 'dingo',\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2V6RcNKz18d",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "And define our own `get_y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4Bq3khmz18d",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# function  that retrieves labels from 'dogs_dict' based on the parent directory name of 'x'.\n",
    "get_y = lambda x: dogs_dict[x.parent.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KzDG6bM4g7b",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now, we can create the `DataBlock` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNAbK1Zsz18d",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "woof = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
    "                 get_items=get_image_files,\n",
    "                 splitter=GrandparentSplitter(train_name='train', valid_name='val'),\n",
    "                 # instead of the parent label, we now use the function from above\n",
    "                 get_y=get_y,\n",
    "                 item_tfms=Resize(460),\n",
    "                 batch_tfms=aug_transforms(size=224, min_scale=0.75)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "45PsN043z18d",
    "outputId": "cb520a57-7aa2-456f-e6a4-84c1a09477d8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dls = woof.dataloaders(path)\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_z3DDt14g7n",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3S0H3VO4g7p",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now we will start training our model. We will use a __convolutional neural network__ backbone and a fully connected head with a single hidden layer as a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/09/cnn.png?raw=true\" style=\"width:100%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/09/convolution.jpg?raw=true\" style=\"width:80%; float:center;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More resources\n",
    "- For an illustrative explanation of convolutional neural networks (CNNs), see [this video](https://www.youtube.com/watch?v=HGwBXDKFk9I&ab_channel=StatQuestwithJoshStarmer). \n",
    "- For a formal coverage, check out the standard book on deep learning [Goodfellow et al (chapter 9)](https://www.academia.edu/download/62266271/Deep_Learning20200303-80130-1s42zvt.pdf), downloadable for free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvfdJuVC4g7p",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The vision_learner\n",
    "\n",
    "This method creates a Learner object from the data object and model inferred from it with the backbone given in `arch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9BTqL7yg4g7q",
    "outputId": "0dc876cb-bacc-488e-f3e9-6e67cbb53d5a",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "learn = vision_learner(dls=dls, arch=models.resnet34, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfbmKODJ4g7s",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- __dls__: Dataloaders\n",
    "- __arch__: architecture.\n",
    "- __metrics__: accuracy\n",
    "- __loss_func__: automatically inferred from `dls`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mentimeter\n",
    "\n",
    "> What kind of loss function would you typically choose for this task? Answer on [Mentimeter](https://www.menti.com/aleeov3mdgtg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing the network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways to build a convolutional neural network. For now, the key idea is this: a particular family of models called **ResNets** works well  in most situations.\n",
    "\n",
    "At this stage, you don’t need to worry about all the architectural details. In practice, your main choice is simply **which size of ResNet to use**. The most common options are:\n",
    "\n",
    "- **ResNet34** – smaller, faster, and often good enough.\n",
    "- **ResNet50** – larger, more accurate, but more computationally expensive.\n",
    "\n",
    "For most tasks early on, deciding between these two is all you really need. (Another choice would be __vision transformers__, but they typically require far more data and compute. ResNets remain widely used because they are simpler, faster, and more reliable on small to medium datasets, making them still an excellent practical default.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dbu8MCUb4g7t",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's print a summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nV6H9b7f4g7t",
    "outputId": "44f81203-62b4-4ae6-86ed-118004faf623",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnqMVrQo4g7v",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Resnet Architecture__\n",
    "\n",
    "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/09/resnet-arch.png?raw=true\" style=\"width:70%\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXBTdjKI4g7w",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Find the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> [Mentimeter](https://www.menti.com/aleeov3mdgtg): How to identify the learning rate using the learning rate finder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUttjTvSaTzX",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Please read the fast.ai [lr_finder docs](https://docs.fast.ai/callback.schedule.html#learner.lr_find). Also, Sylvain Gugger from the fast.ai team wrote a nice [blog post](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html) on how to find  good learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "tavTRz7x4g7w",
    "outputId": "c5161276-1ea2-48c6-f4de-76c515302115",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCHc3NyPz18d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The learning rate finder does the following [(adapted from here)](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html):\n",
    "* Over an epoch begin your SGD with a very low learning rate (like 10−8) but change it (by multiplying it by a certain factor for instance) at each mini-batch until it reaches a very high value (like 1 or 10).\n",
    "* Record the loss each time at each iteration and once you're finished, plot those losses against the learning rate. You'll find something like this:\n",
    "\n",
    "* Plot of the loss against the learning rate: The loss decreases at the beginning, then it stops and it goes back increasing, usually extremely quickly. That's because with very low learning rates, we get better and better, especially since we increase them.\n",
    "* Then comes a point where we reach a value that's too high and learning process starts to diverge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSWymmyjz18e",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Looking at the previous graph, what is the best learning rate to choose?\n",
    "\n",
    "* __Not the one corresponding to the minimum.__\n",
    "\n",
    "Why? Well the learning rate that corresponds to the minimum value is already a bit too high, since we are at the edge between improving and getting all over the place. We want to go one order of magnitude before, a value that's still aggressive (so that we train quickly) but still on the safe side from an explosion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZtV3eXF4g72",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fit the model\n",
    "\n",
    "Fit the model based on selected learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "DRDLUuRe4g72",
    "outputId": "a33590a8-7d68-49b3-ecd1-4f2ee80adf07",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, lr_max= 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCr_FScLz18e",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We train using the __one cycle policy__. Over the duration of the training process, the policy goes from a lower learning rate to a higher one than going back to the minimum. The maximum learning rate is the value picked with the Learning Rate Finder, and the lower one can be ten times lower. It has been proven to be effective and efficient. See [here](https://sgugger.github.io/the-1cycle-policy.html) for an excellent explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVOBZHDQ4g75",
    "outputId": "448f77a4-3429-4644-e16e-5eb780982eb1",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBho1XVmbdin",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So far we have fitted 2 epochs and it ran pretty quickly. Why is that so? Because we used a little trick (called transfer learning).\n",
    "\n",
    "What did we do?\n",
    "We added a few extra layers at the end of architecture, and we only trained those. We left most of the early layers as they were. This is called freezing layers i.e weights of the layers.\n",
    "\n",
    "- When we call fit or `fit_one_cycle()` on a create_cnn, it will just fine-tune these extra layers at the end, and run very fast.\n",
    "- To get a better model, we have to call `unfreeze()` to train the whole model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YDA_oIW4g78",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unfreeze and train again\n",
    "\n",
    "Since our model is working as we expect it to, we will unfreeze our model and train some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5uTKDT04g79",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "vKINuvlY4g7_",
    "outputId": "b6ae6c4a-5264-40ed-be37-d5da6edab373",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeRf6bCs6rY6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Learning rates after unfreezing__\n",
    "\n",
    "The basic rule of thumb is after you unfreeze (i.e. train the whole thing), pass a max learning rate parameter, pass it a slice, make the second part of that slice about 10 times smaller than your first stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "o1OPJCKP4g8H",
    "outputId": "15e1216e-bfdf-4be4-fe2c-3b308998e45c",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lr_max=slice(1e-5,1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mentimeter\n",
    "\n",
    "> Which next steps would you undertake based on these learning results? Answer on [Mentimeter](https://www.menti.com/aleeov3mdgtg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "id1AtBh-z18e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Need for Different Learning Rates:**\n",
    "  - Early layers (from pre-trained model) need less adjustment.\n",
    "  - Later layers (trained for specific task) require more significant learning.\n",
    "  - High learning rates on early layers can disrupt pre-learned features.\n",
    "\n",
    "- **The Rule of Thumb:**\n",
    "  - Use differential learning rates after unfreezing the model.\n",
    "  - __Lower__ rate for __early__ layers, __higher__ rate for __later__ layers.\n",
    "  - In FastAI, use a \"slice\" to specify rates: later layers' rate ~10x the early layers'.\n",
    "\n",
    "- **Fine-Tuning and Stability:**\n",
    "  - Approach ensures stable, effective fine-tuning across the model.\n",
    "  - Early layers receive small updates to adjust learned features.\n",
    "  - Later layers undergo more rapid learning with task-specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fUTZdUX4g8M",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "If the model overfits we can reload stage 1 and train the model again for fewer epochs or another learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QxGWQBt4g8N",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Loads the model state previously saved under the name 'stage-1'.\n",
    "learn = learn.load('stage-1')\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "G5vtSxstZW-j",
    "outputId": "db52d527-434b-4b47-e1ab-989e6d983be2",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, lr_max=slice(1e-5,1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6RSRMXe4g8P",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzqHFBvPc7fo",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "It’s important to see what comes out of our model. We have seen one way of what goes in, now let’s see what our model has predicted.\n",
    "\n",
    "The `ClassificationInterpretation` class has methods for creating confusion matrix as well as plotting misclassified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "0Wti_ljc4g8Q",
    "outputId": "8153a61b-89fd-4490-cdcb-08f25e20d639",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DHDHLEa4g8P",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Top Losses\n",
    "\n",
    "We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6UqiX5NgQl9r",
    "outputId": "84cd31c8-e998-4ae4-9c67-f10b11cd44f3",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "interp.plot_top_losses(9, largest=True, figsize=(18,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfD5VK8d4g8T",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Furthermore, when we plot the confusion matrix. Interestingly, the model often confuses English Foxhounds with Beagles. This confirmes that our model works very well until a certain level as these two dog breeds look very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "id": "5XFcBqEE4g8T",
    "outputId": "24674601-6e6e-4aab-aafd-f99c4f5cbc0c",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roFnT8fd4g8W",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Most Confused__\n",
    "\n",
    "Sorted descending list of largest non-diagonal entries of confusion matrix, presented as actual, predicted, number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "id": "gdqVti_y4g8Y",
    "outputId": "df07384d-222d-4cd5-c3b1-58b5db835481",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    " interp.most_confused(min_val=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROXoZ8PWMJBk",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercises: Explainability of The Model using Grad-CAM\n",
    "\n",
    "After training a model, you obtain the classification results of given images. Have you ever wondered which part of an image the model focuses on when making a prediction about its class? In this exercise, you have a chance to investigate [Grad-CAM](https://arxiv.org/abs/1610.02391), a visualization technique that helps to look into the captured features of individual layers of a convolutional neural nework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWv8JNINNgS6",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## What is Grad-CAM?\n",
    "\n",
    "Gradient-weighted Class Activation Mapping (Grad-CAM) is a technique used to visualize and understand what parts of an image a deep learning model focuses on when making predictions. It works by computing the gradients of the target class score with respect to the feature maps of a specific convolutional layer. These gradients highlight the importance of each region in the feature maps, which are then combined and overlaid onto the original image to produce a heatmap. This heatmap shows which regions of the image contributed the most to the model's decision, making it easier to interpret and debug the model's behavior.\n",
    "\n",
    "In simple words, Grad-CAM helps us understand a model's decision-making process by highlighting which features in specific layers contribute most to a prediction. To use it effectively, we typically focus on deeper convolutional layers where high-level features are learned, and analyze how these features correlate with the model's output. This insight can guide debugging, model refinement, or identifying biases in the predictions. In this exercise, we will take a look at the final convolutional layer of the `resnet34` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCxxgu_3N2sb",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Task 1: Identify the final convolutional layer of ResNet34\n",
    "\n",
    "After training the model as instructed in the lecture, we want to take a look at the feature maps in the final block of convolutional layers. The goal of Grad-CAM is to take the output feature maps from the last CNN block and figure out which parts are most important for predicting a specific class. To do this, we calculate the average gradient of each feature map with respect to the class score, which gives us their importance. Then, we use these weights to combine the feature maps, highlighting the areas in the image that influenced the prediction.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Take a look at the model summary again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0vu8i_fmMIS2",
    "outputId": "8e4b5fa1-4a91-4246-e3ee-2ab8d2533a5a",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ihB1N77RGWn",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/09/resnet-arch.png?raw=true\" style=\"width:70%\" />\n",
    "\n",
    "Now, take out the final convolutional block.\n",
    "\n",
    "Hint 1: It should be in the form of `learn.model[0][7][final_block]`\n",
    "\n",
    "Hint 2: First index [0] is to access to the CNN layers. It is built based on a `Sequential` class in `PyTorch`. You can print `learn.model` to see the structure of the model. `learn.model[1]` allows you to access to the linear layers (the avg pool + white block `fc 1000` in the figure.\n",
    "\n",
    "Hint 3: Second index is to access to the second nested `Sequential` block. You can find the required index by printing `learn.model[0][7]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbuqsxPKOqk2",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Your Your Code Here\n",
    "last_cnn_block = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyDFInGERg2W",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We create the heatmap by linearly combining the feature maps, inspired by a [scaled-down](https://github.com/henripal/maps/blob/master/nbs/big_resnet50-interpret-gradcam-dogs.ipynb) version of the gradients as weights.In FastAI, a [Hook](https://docs.fast.ai/callback.hook.html) is a utility that allows you to \"hook into\" the internal workings of a model during the forward or backward pass. It lets you capture or modify the intermediate outputs, gradients, or other data flowing through specific layers of a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGyii7Q4Rg_F",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# ---- No need to touch this code, only run it ----\n",
    "import fastai\n",
    "\n",
    "# Hook for the feature maps\n",
    "fmap_hook = Hook(last_cnn_block, lambda m, i, o: o)\n",
    "\n",
    "# Hook for the gradients\n",
    "def gradient_torch_hook(module, grad_input, grad_output):\n",
    "    return grad_output[0]\n",
    "\n",
    "gradient_hook = Hook(last_cnn_block, gradient_torch_hook, is_forward=False)\n",
    "\n",
    "# Convert image tensor to numpy for visualization\n",
    "def image_from_tensor(imagetensor):\n",
    "    numpied = imagetensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    numpied -= numpied.min()\n",
    "    numpied /= numpied.max()\n",
    "    return numpied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dER64i7QVK0g",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 2. Task 2: Extract images from the first batch  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCaEZt7oWP1D",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, let's load the first batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRZPK0qaVgn_",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "# Hint: use first() from fastai\n",
    "image_batch, label_batch = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVszXUdHWTIu",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Next, set the model into evaluation mode because we are only interested in the forward-pass computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25GpiWJuWM9R",
    "outputId": "49404198-d746-462c-c250-1a01773f48f7",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Your Code Here\n",
    "# Hint 1: set `learn' model to eval() mode\n",
    "# Hint 2: check out https://docs.fast.ai/tutorial.imagenette.html#training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XakCQI7BWuxu",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Finally, we compute the gradients and visualize several images. Your final task is to fill in 3 lines of code:\n",
    "* set up the for loop with `16` images,\n",
    "* set the model into `zero_grad()` [mode](https://datascience.stackexchange.com/questions/124487/what-do-we-mean-by-optimizer-zero-grad#:~:text=We%20need%20to%20invoke%20zero_grad,gradients%20to%20update%20the%20weights.), and\n",
    "* perform a `forward pass`.\n",
    "\n",
    "We’ve introduced deep learning at a high level, but now you’re seeing how to interact with the model at a deeper level by exploring the forward and backward passes. If you’d like to learn more about how a training pipeline works in PyTorch, check out this [guide](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "H9xeFf6IUFBE",
    "outputId": "aa5a252a-d8c5-45a6-ca74-5fa398bbea5f",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Setup for visualization\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "fig.tight_layout()\n",
    "\n",
    "# [1] Your Code Here\n",
    "# Define 16 images to use\n",
    "for i in range(16):\n",
    "    image =   # Add batch dimension\n",
    "    label =   # Add batch dimension\n",
    "\n",
    "    # [2] Your Code Here\n",
    "    # Perform zero_grad here\n",
    "    # Hint: check out https://docs.fast.ai/tutorial.imagenette.html#training\n",
    "    \n",
    "\n",
    "    # [3] Your Code Here\n",
    "    # Run the forward pass here using \"image\" defined above\n",
    "    # Hint: call learn.model(input)\n",
    "    out = \n",
    "\n",
    "    # Create one-hot encoding for the predicted class\n",
    "    onehot = torch.zeros_like(out)\n",
    "    onehot[0, torch.argmax(out)] = 1.0\n",
    "\n",
    "    # Backpropagate from the one-hot encoded output\n",
    "    out.backward(gradient=onehot, retain_graph=True)\n",
    "\n",
    "    # Retrieve gradients and feature maps\n",
    "    gradients = gradient_hook.stored\n",
    "    gradients = gradients[0].detach().cpu().numpy()\n",
    "    fmaps = fmap_hook.stored\n",
    "    fmaps = fmaps[0].detach().cpu().numpy()\n",
    "\n",
    "    # Process gradients and feature maps\n",
    "    gradient_linearization = gradients.sum((1, 2))  # Sum across width and height\n",
    "    weighted_fmaps = (fmaps * gradient_linearization[:, None, None]).sum(0)\n",
    "\n",
    "    # Normalize heatmap\n",
    "    heatmap = np.maximum(weighted_fmaps, 0)\n",
    "    heatmap /= heatmap.max()\n",
    "\n",
    "    # Resize heatmap to match input image size\n",
    "    upsampled = cv2.resize(heatmap, (image.shape[2], image.shape[3]))\n",
    "\n",
    "    # Display the image and heatmap\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    ax.imshow(image_from_tensor(image[0]))\n",
    "    ax.imshow(upsampled, alpha=0.6, cmap='jet')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNyFneOYaRpB",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "That's it! If you are interested in more fine-grained heatmaps, consider adapting this [code](https://github.com/henripal/maps/blob/master/nbs/big_resnet50-interpret-gradcam-dogs.ipynb) into this exercise! You might have to adjust the code from `fastaiv1` to `fastaiv2` to make it compatible to this notebook's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCcBOXZwz18e",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<img src=\"https://github.com/GuntherGust/tds2_data/blob/main/images/d3.png?raw=true\" style=\"width:50%; float:center;\" />"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "rise": {
   "enable_chalkboard": false,
   "overlay": "<div class='background'></div><div class='header'>WS 23/24</br>TDS 2</div><div class='logo'><img src='images/d3logo.png'></div><div class='bar'></div>",
   "scroll": true,
   "slideNumber": "h.v"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
