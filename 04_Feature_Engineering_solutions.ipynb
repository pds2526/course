{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/pds2425/course/blob/main/notebooks/03_Feature_Engineering.ipynb\n",
    "\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "<div class='bar_title'></div>\n",
    "\n",
    "*Practical Data Science*\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "Gunther Gust<br>\n",
    "Chair for Enterprise AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/d3.png\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/CAIDASlogo.png\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [],
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Loading-the-Data\" data-toc-modified-id=\"Loading-the-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Loading the Data</a></span></li><li><span><a href=\"#Select-Variables-and-Split-Dataset\" data-toc-modified-id=\"Select-Variables-and-Split-Dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Select Variables and Split Dataset</a></span></li><li><span><a href=\"#Feature-Engineering-on-Numeric-Data\" data-toc-modified-id=\"Feature-Engineering-on-Numeric-Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature Engineering on Numeric Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Normalization\" data-toc-modified-id=\"Normalization-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Normalization</a></span></li><li><span><a href=\"#Standardization\" data-toc-modified-id=\"Standardization-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Standardization</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Summary</a></span></li></ul></li><li><span><a href=\"#Binarization\" data-toc-modified-id=\"Binarization-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Binarization</a></span></li><li><span><a href=\"#Binning\" data-toc-modified-id=\"Binning-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Binning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fixed-Width-Binning\" data-toc-modified-id=\"Fixed-Width-Binning-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Fixed-Width Binning</a></span></li><li><span><a href=\"#Adaptive-Binning\" data-toc-modified-id=\"Adaptive-Binning-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Adaptive Binning</a></span></li></ul></li><li><span><a href=\"#Statistical-Transformations\" data-toc-modified-id=\"Statistical-Transformations-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Statistical Transformations</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering-on-Categorical-Data\" data-toc-modified-id=\"Feature-Engineering-on-Categorical-Data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature Engineering on Categorical Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Label-and-One-Hot-Encoding\" data-toc-modified-id=\"Label-and-One-Hot-Encoding-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Label and One-Hot-Encoding</a></span></li><li><span><a href=\"#Count-Encodings\" data-toc-modified-id=\"Count-Encodings-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Count Encodings</a></span></li><li><span><a href=\"#Target-Encodings\" data-toc-modified-id=\"Target-Encodings-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Target Encodings</a></span></li><li><span><a href=\"#CatBoost-Encoding\" data-toc-modified-id=\"CatBoost-Encoding-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>CatBoost Encoding</a></span></li><li><span><a href=\"#Warning\" data-toc-modified-id=\"Warning-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Warning</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Credits__\n",
    "\n",
    "Parts of the material of this lecture are adopted from www.kaggle.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**This lecture provides an overview on different feature engineering techniques.**\n",
    "\n",
    "Starting with a baseline dataset, we will\n",
    "\n",
    "- modify existing variables \n",
    "- add additional features to  our dataset \n",
    "- train a predictive model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Feature engineering** is an essential part of building a powerful predictive model. \n",
    "\n",
    "Each problem is domain specific and better features (suited to the problem) are often the deciding factor of the performance of your system. \n",
    "\n",
    "Feature Engineering requires experience as well as creativity and this is the reason **Data Scientists often spend the majority of their time** in the data preparation phase before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "_\"Coming up with features is difficult, time-consuming, requires expert knowledge. Applied machine learning is basically feature engineering.\"_\n",
    "\n",
    "Prof. Andrew Ng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_\"Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\"_\n",
    "\n",
    "Dr. Jason Brownlee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_\"At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\"_\n",
    "\n",
    "Prof. Pedro Domingos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading the Data\n",
    "This week, we will work with a sample of the [adult dataset](https://archive.ics.uci.edu/ml/datasets/adult) which has some __census information on individuals__. We'll use it to train a model to predict whether __salary is greater than 50k USD or not.__ Again, our first step is to load and familiarize ourself with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>101320</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>1902</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>236746</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>10520</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>96185</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>112847</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>82297</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;50k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt     education  education-num  \\\n",
       "0   49            Private  101320    Assoc-acdm           12.0   \n",
       "1   44            Private  236746       Masters           14.0   \n",
       "2   38            Private   96185       HS-grad            NaN   \n",
       "3   38       Self-emp-inc  112847   Prof-school           15.0   \n",
       "4   42   Self-emp-not-inc   82297       7th-8th            NaN   \n",
       "\n",
       "        marital-status        occupation    relationship                 race  \\\n",
       "0   Married-civ-spouse               NaN            Wife                White   \n",
       "1             Divorced   Exec-managerial   Not-in-family                White   \n",
       "2             Divorced               NaN       Unmarried                Black   \n",
       "3   Married-civ-spouse    Prof-specialty         Husband   Asian-Pac-Islander   \n",
       "4   Married-civ-spouse     Other-service            Wife                Black   \n",
       "\n",
       "       sex  capital-gain  capital-loss  hours-per-week  native-country salary  \n",
       "0   Female             0          1902              40   United-States  >=50k  \n",
       "1     Male         10520             0              45   United-States  >=50k  \n",
       "2   Female             0             0              32   United-States   <50k  \n",
       "3     Male             0             0              40   United-States  >=50k  \n",
       "4   Female             0             0              50   United-States   <50k  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'https://raw.githubusercontent.com/GuntherGust/tds2_data/main/data/adult.csv'\n",
    "adult_data = pd.read_csv(file_path)\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Select Variables and Split Dataset\n",
    "\n",
    "Before we start to engineer new features, we select the feature and target variables. \n",
    "\n",
    "The (binary) variable ``salary`` describes if a person earns more or less that \\\\$50k. We replace the labels with numeric values (0: Salary < \\\\$50k, 1: Salary > \\\\$50k) and subsequently select it as our target variable y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt     education  education-num  \\\n",
      "0   49            Private  101320    Assoc-acdm           12.0   \n",
      "1   44            Private  236746       Masters           14.0   \n",
      "2   38            Private   96185       HS-grad            NaN   \n",
      "3   38       Self-emp-inc  112847   Prof-school           15.0   \n",
      "4   42   Self-emp-not-inc   82297       7th-8th            NaN   \n",
      "\n",
      "        marital-status        occupation    relationship                 race  \\\n",
      "0   Married-civ-spouse               NaN            Wife                White   \n",
      "1             Divorced   Exec-managerial   Not-in-family                White   \n",
      "2             Divorced               NaN       Unmarried                Black   \n",
      "3   Married-civ-spouse    Prof-specialty         Husband   Asian-Pac-Islander   \n",
      "4   Married-civ-spouse     Other-service            Wife                Black   \n",
      "\n",
      "       sex  capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
      "0   Female             0          1902              40   United-States       1  \n",
      "1     Male         10520             0              45   United-States       1  \n",
      "2   Female             0             0              32   United-States       0  \n",
      "3     Male             0             0              40   United-States       1  \n",
      "4   Female             0             0              50   United-States       0  \n"
     ]
    }
   ],
   "source": [
    "adult_data = adult_data.assign(salary=(adult_data['salary']=='>=50k').astype(int))\n",
    "print(adult_data.head())\n",
    "\n",
    "y = adult_data['salary']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The remaining columns serve as our features X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = adult_data.drop('salary', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we perform a train-test split to train and evaluate our machine learning models for the model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we are ready to start preparing and enhancing our numerical and categorical features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Engineering on Numeric Data\n",
    "\n",
    "By Numeric data we mean continuous data and not discrete data which is typically represented as categorical data. Integers and floats are the most common and widely used numeric data types for continuous numeric data. Even though numeric data can be directly fed into machine learning models, we still have to engineer and preprocess features which are relevant to the scenario, problem, domain and machine learning model.\n",
    "\n",
    "To this end, we can distinguish between preprocessing and feature generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To work on our numeric features, we have to identify all numeric columns in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'fnlwgt',\n",
       " 'education-num',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'hours-per-week']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numCols = [cname for cname in train_X.columns if train_X[cname].dtype != \"object\"]\n",
    "numCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To avoid problems with missing values we use a ``SimpleImputer`` for the numeric columns before we continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "simple_imputer = SimpleImputer()\n",
    "\n",
    "train_X_num = pd.DataFrame(simple_imputer.fit_transform(train_X[numCols]), columns=numCols, index=train_X.index)\n",
    "val_X_num = pd.DataFrame(simple_imputer.transform(val_X[numCols]), columns=numCols, index=val_X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Our dataset may contain attributes with a mixture of scales for various quantities. However, many machine learning methods require or at least are more effective if the data attributes have the __same scale.__ \n",
    "\n",
    "For example, ``capital gain`` and ``capital loss`` is measured in USD while age is measured in years in our dataset at hand.\n",
    "\n",
    "To avoid having numeric values from different scales we can use two popular data scaling methods: __normalization and standardization.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Normalization\n",
    "\n",
    "Normalization refers to rescaling numeric attributes into the __range between 0 and 1.__ It is useful to scale the input attributes for a model that relies on the magnitude of values, such as distance measures used in k-nearest neighbors and in the preparation of coefficients in regression.\n",
    "\n",
    "Using Scikit-learn's ``MinMaxScaler`` we can rescale an attribute according to the following formula:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    X = \\frac{(X - min(X))}{(max(X) - min(X))}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26464</th>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.215102</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.118419</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.038438</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.346939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.128628</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>0.205479</td>\n",
       "      <td>0.064474</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13123</th>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.093175</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.244898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19648</th>\n",
       "      <td>0.205479</td>\n",
       "      <td>0.176642</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9845</th>\n",
       "      <td>0.315068</td>\n",
       "      <td>0.039635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799</th>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.011654</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.142264</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.03325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.448980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24420 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "26464  0.136986  0.215102       0.533333       0.00000           0.0   \n",
       "16134  0.054795  0.118419       0.600000       0.00000           0.0   \n",
       "4747   0.465753  0.038438       0.066667       0.00000           0.0   \n",
       "8369   0.095890  0.128628       0.533333       0.00000           0.0   \n",
       "5741   0.205479  0.064474       0.800000       0.00000           0.0   \n",
       "...         ...       ...            ...           ...           ...   \n",
       "13123  0.041096  0.093175       0.600000       0.00000           0.0   \n",
       "19648  0.205479  0.176642       0.733333       0.00000           0.0   \n",
       "9845   0.315068  0.039635       0.000000       0.00000           0.0   \n",
       "10799  0.150685  0.011654       0.533333       0.00000           0.0   \n",
       "2732   0.109589  0.142264       0.533333       0.03325           0.0   \n",
       "\n",
       "       hours-per-week  \n",
       "26464        0.397959  \n",
       "16134        0.193878  \n",
       "4747         0.346939  \n",
       "8369         0.602041  \n",
       "5741         0.397959  \n",
       "...               ...  \n",
       "13123        0.244898  \n",
       "19648        0.397959  \n",
       "9845         0.193878  \n",
       "10799        0.397959  \n",
       "2732         0.448980  \n",
       "\n",
       "[24420 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_X_num_normalized = pd.DataFrame(scaler.fit_transform(train_X_num), \n",
    "                                      columns=train_X_num.columns, index=train_X_num.index)\n",
    "val_X_num_normalized = pd.DataFrame(scaler.transform(val_X_num), \n",
    "                                    columns=train_X_num.columns, index=val_X_num.index)\n",
    "\n",
    "train_X_num_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Standardization\n",
    "\n",
    "In contrast to normalization, we could also use standardization for our numerical variables. In this context, standardization refers to shifting the distribution of each attribute to have a __mean of zero__ and __a standard deviation of one.__ It is useful to standardize attributes for a model that assumes a __similar distribution__ (mean and variance) of attributes such as Gaussian processes.\n",
    "\n",
    "Using Scikit-learn's ```StandardScaler``` we can rescale an attribute according to the following formula:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    X = \\frac{(X - mean(X))}{\\sqrt{var(X)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26464</th>\n",
       "      <td>-0.853017</td>\n",
       "      <td>1.316410</td>\n",
       "      <td>-0.420746</td>\n",
       "      <td>-0.146391</td>\n",
       "      <td>-0.217388</td>\n",
       "      <td>-0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>-1.292554</td>\n",
       "      <td>-0.027373</td>\n",
       "      <td>-0.029346</td>\n",
       "      <td>-0.146391</td>\n",
       "      <td>-0.217388</td>\n",
       "      <td>-1.652429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>0.905131</td>\n",
       "      <td>-1.139029</td>\n",
       "      <td>-3.160546</td>\n",
       "      <td>-0.146391</td>\n",
       "      <td>-0.217388</td>\n",
       "      <td>-0.440408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>-1.072786</td>\n",
       "      <td>0.114522</td>\n",
       "      <td>-0.420746</td>\n",
       "      <td>-0.146391</td>\n",
       "      <td>-0.217388</td>\n",
       "      <td>1.579628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>-0.486737</td>\n",
       "      <td>-0.777155</td>\n",
       "      <td>1.144853</td>\n",
       "      <td>-0.146391</td>\n",
       "      <td>-0.217388</td>\n",
       "      <td>-0.036400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "26464 -0.853017  1.316410      -0.420746     -0.146391     -0.217388   \n",
       "16134 -1.292554 -0.027373      -0.029346     -0.146391     -0.217388   \n",
       "4747   0.905131 -1.139029      -3.160546     -0.146391     -0.217388   \n",
       "8369  -1.072786  0.114522      -0.420746     -0.146391     -0.217388   \n",
       "5741  -0.486737 -0.777155       1.144853     -0.146391     -0.217388   \n",
       "\n",
       "       hours-per-week  \n",
       "26464       -0.036400  \n",
       "16134       -1.652429  \n",
       "4747        -0.440408  \n",
       "8369         1.579628  \n",
       "5741        -0.036400  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_X_num_standardized = pd.DataFrame(scaler.fit_transform(train_X_num), \n",
    "                                        columns=train_X_num.columns, index=train_X_num.index)\n",
    "val_X_num_standardized = pd.DataFrame(scaler.transform(val_X_num), \n",
    "                                      columns=train_X_num.columns, index=val_X_num.index)\n",
    "\n",
    "train_X_num_standardized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Summary\n",
    "\n",
    "Data rescaling is an important part of data preparation before applying machine learning algorithms. However, it is __hard to know__ whether normalization or standardization of the data will improve the performance of a predictive model __in advance.__ \n",
    "\n",
    "A good tip for a practical application is to create __rescaled copies__ of your dataset and evaluate them against each other. This process can quickly show which rescaling method will improve your selected models in the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Binarization\n",
    "\n",
    "For some problems raw frequencies or counts may not be relevant for building a model. In these cases it is only relevant if a numeric value exceeds a specific threshold (e.g. a person is at least 40 years old). Hence we do not require the number of times the action was performed but only a binary feature.\n",
    "\n",
    "We can binarize a feature using Scikit-learn's ``Binarizer`` function (Note that we use the raw dataset for this example - clearly we could normalize or standardize the dataframe afterwards):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>40Plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26464</th>\n",
       "      <td>27.0</td>\n",
       "      <td>329005.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>21.0</td>\n",
       "      <td>186648.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>51.0</td>\n",
       "      <td>68882.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>24.0</td>\n",
       "      <td>201680.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>32.0</td>\n",
       "      <td>107218.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "26464  27.0  329005.0            9.0           0.0           0.0   \n",
       "16134  21.0  186648.0           10.0           0.0           0.0   \n",
       "4747   51.0   68882.0            2.0           0.0           0.0   \n",
       "8369   24.0  201680.0            9.0           0.0           0.0   \n",
       "5741   32.0  107218.0           13.0           0.0           0.0   \n",
       "\n",
       "       hours-per-week  40Plus  \n",
       "26464            40.0     0.0  \n",
       "16134            20.0     0.0  \n",
       "4747             35.0     1.0  \n",
       "8369             60.0     0.0  \n",
       "5741             40.0     0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "train_X_binary_age = train_X_num.copy()\n",
    "val_X_binary_age = val_X_num.copy()\n",
    "\n",
    "binarizer = Binarizer(threshold=40)\n",
    "\n",
    "train_X_binary_age['40Plus'] = binarizer.transform([train_X_binary_age['age']])[0]\n",
    "val_X_binary_age['40Plus'] = binarizer.transform([val_X_binary_age['age']])[0]\n",
    "\n",
    "train_X_binary_age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Binning\n",
    "\n",
    "The problem of working with raw, numeric features is that often the distribution of values in these features will be skewed. This signifies that some values will occur quite frequently while some will be quite rare. Hence there are strategies to deal with this, which include binning. \n",
    "\n",
    "Binning is used for transforming continuous numeric features into discrete ones. These discrete values can be interpreted as categories or bins into which the raw values are grouped into. Each group represents a specific degree of intensity and hence a specific range of continuous numeric values fall into it.\n",
    "\n",
    "Let's again use the age variable to perform two different types of binning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Fixed-Width Binning\n",
    "\n",
    "In fixed-width binning, specific fixed widths for each bin are defined by the user. Each bin has a fixed range of values which should be assigned to that bin on the basis of some domain knowledge.\n",
    "\n",
    "We can use Pandas ```cut``` function to bin the age into predefined groups and assign labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>AgeBinned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26464</th>\n",
       "      <td>27.0</td>\n",
       "      <td>329005.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>21.0</td>\n",
       "      <td>186648.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>51.0</td>\n",
       "      <td>68882.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>24.0</td>\n",
       "      <td>201680.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>32.0</td>\n",
       "      <td>107218.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "26464  27.0  329005.0            9.0           0.0           0.0   \n",
       "16134  21.0  186648.0           10.0           0.0           0.0   \n",
       "4747   51.0   68882.0            2.0           0.0           0.0   \n",
       "8369   24.0  201680.0            9.0           0.0           0.0   \n",
       "5741   32.0  107218.0           13.0           0.0           0.0   \n",
       "\n",
       "       hours-per-week AgeBinned  \n",
       "26464            40.0         1  \n",
       "16134            20.0         0  \n",
       "4747             35.0         1  \n",
       "8369             60.0         0  \n",
       "5741             40.0         1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_bin_age = train_X_num.copy()\n",
    "val_X_bin_age = val_X_num.copy()\n",
    "\n",
    "bin_ranges = [0, 25, 60, 999]\n",
    "bin_labels = [0, 1, 2]\n",
    "\n",
    "train_X_bin_age['AgeBinned'] = pd.cut(train_X_bin_age['age'], \n",
    "                                      bins=bin_ranges, labels=bin_labels)\n",
    "val_X_bin_age['AgeBinned'] = pd.cut(val_X_bin_age['age'], \n",
    "                                    bins=bin_ranges, labels=bin_labels)\n",
    "\n",
    "train_X_bin_age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Adaptive Binning\n",
    "\n",
    "The major drawback in using fixed-width binning is __unbalanced bin sizes.__ As we manually decide the bin ranges, we can end up with irregular bins which are not uniform based on the number of data points. Some bins (such as \"young (0)\" and \"old (2)\") might be sparsely populated while some (such as \"medium (1)\") are densely populated.\n",
    "\n",
    "To overcome this issues we can use adaptive binning based on the distribution of the data.\n",
    "\n",
    "To cut the space into equal partitions we can use the __quantiles as cut-points:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>AgeBinned</th>\n",
       "      <th>AgeBinnedAdaptive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26464</th>\n",
       "      <td>27.0</td>\n",
       "      <td>329005.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>21.0</td>\n",
       "      <td>186648.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>51.0</td>\n",
       "      <td>68882.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>24.0</td>\n",
       "      <td>201680.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>32.0</td>\n",
       "      <td>107218.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "26464  27.0  329005.0            9.0           0.0           0.0   \n",
       "16134  21.0  186648.0           10.0           0.0           0.0   \n",
       "4747   51.0   68882.0            2.0           0.0           0.0   \n",
       "8369   24.0  201680.0            9.0           0.0           0.0   \n",
       "5741   32.0  107218.0           13.0           0.0           0.0   \n",
       "\n",
       "       hours-per-week AgeBinned AgeBinnedAdaptive  \n",
       "26464            40.0         1                 0  \n",
       "16134            20.0         0                 0  \n",
       "4747             35.0         1                 2  \n",
       "8369             60.0         0                 0  \n",
       "5741             40.0         1                 1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_list = [0, 0.33, 0.66, 1]\n",
    "quantile_labels = [0, 1, 2]\n",
    "\n",
    "train_X_bin_age['AgeBinnedAdaptive'] = pd.qcut(train_X_bin_age['age'], \n",
    "                                               q=quantile_list, labels=quantile_labels)\n",
    "val_X_bin_age['AgeBinnedAdaptive'] = pd.qcut(val_X_bin_age['age'],   \n",
    "                                             q=quantile_list, labels=quantile_labels)\n",
    "\n",
    "train_X_bin_age.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Statistical Transformations\n",
    "\n",
    "Many variables, such as ``capital-gain`` or ``fnlwgt`` (sampling weight) span several orders of magnitude. While the vast majority of persons has very small capital-gains, a few people have very high gains. To work with such skewed variables we can use the log transformation. \n",
    "\n",
    "__Log transforms__ are useful when applied to skewed distributions as they tend to expand the values which fall in the range of lower magnitudes and tend to compress or reduce the values which fall in the range of higher magnitudes. This tends to make the skewed distribution as normal-like as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X_logfnlwgt = train_X_num.copy()\n",
    "val_X_logfnlwgt = val_X_num.copy()\n",
    "\n",
    "train_X_logfnlwgt['logfnlwgt'] = np.log1p(train_X_logfnlwgt['fnlwgt'])\n",
    "val_X_logfnlwgt['logfnlwgt'] = np.log1p(val_X_logfnlwgt['fnlwgt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can see this effect plotting both histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "train_X_logfnlwgt[['fnlwgt', 'logfnlwgt']].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_X_logfnlwgt_std = pd.DataFrame(scaler.fit_transform(train_X_logfnlwgt), \n",
    "                                        columns=train_X_logfnlwgt.columns, index=train_X_logfnlwgt.index)\n",
    "val_X_logfnlwgt_std = pd.DataFrame(scaler.transform(val_X_logfnlwgt), \n",
    "                                      columns=val_X_logfnlwgt.columns, index=val_X_logfnlwgt.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "We can train support vector machines (support vector classifiers, ``SVC``) using the different datasets and feature engineering techniques to evaluate their impact on the model performance. Note that we could (and should) combine these techniques to train powerful models and apply them in real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def score_dataset_svc(X_train, X_valid, y_train, y_valid):\n",
    "    model = SVC(gamma='auto', random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Features: 0.7558039552880481\n",
      "Binary Age: 0.7560496253531507\n",
      "Binned Age: 0.7560496253531507\n",
      "Log FNLWGT: 0.7560496253531507\n",
      "Normalized Features: 0.800761577201818\n",
      "Standardized Features: 0.8204151824100233\n",
      "Log FNLWGT_std: 0.8191868320845105\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw Features: {}\".\n",
    "      format(score_dataset_svc(train_X_num, val_X_num, train_y, val_y)))\n",
    "print(\"Binary Age: {}\".format(score_dataset_svc(train_X_binary_age, val_X_binary_age, train_y, val_y)))\n",
    "print(\"Binned Age: {}\".format(score_dataset_svc(train_X_bin_age, val_X_bin_age, train_y, val_y)))\n",
    "print(\"Log FNLWGT: {}\".format(score_dataset_svc(train_X_logfnlwgt, val_X_logfnlwgt, train_y, val_y)))\n",
    "\n",
    "print(\"Normalized Features: {}\".\n",
    "      format(score_dataset_svc(train_X_num_normalized, val_X_num_normalized, train_y, val_y)))\n",
    "print(\"Standardized Features: {}\".\n",
    "      format(score_dataset_svc(train_X_num_standardized, val_X_num_standardized, train_y, val_y)))\n",
    "\n",
    "print(\"Log FNLWGT_std: {}\".format(score_dataset_svc(train_X_logfnlwgt_std, val_X_logfnlwgt_std, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Exercises\n",
    "1. Replace `SVC` model with the `RandomForestClassifier` model and modify the `score_dataset` function to retrain the model for the following datasets: Raw Features, Normalized Features and Standardized Features. Hint: import the model in this [scikit-learn documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html). \n",
    "\n",
    "2. Can you explain why the results do not considerably differ between the datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def score_dataset_rf(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestClassifier(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Features: 0.8077631740572411\n",
      "Binary Age: 0.8061663186340744\n",
      "Binned Age: 0.8024812676575359\n",
      "Log FNLWGT: 0.8178356467264464\n",
      "Normalized Features: 0.8080088441223436\n",
      "Standardized Features: 0.8066576587642795\n",
      "Log FNLWGT_std: 0.8168529664660361\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw Features: {}\".\n",
    "      format(score_dataset_rf(train_X_num, val_X_num, train_y, val_y)))\n",
    "print(\"Binary Age: {}\".format(score_dataset_rf(train_X_binary_age, val_X_binary_age, train_y, val_y)))\n",
    "print(\"Binned Age: {}\".format(score_dataset_rf(train_X_bin_age, val_X_bin_age, train_y, val_y)))\n",
    "print(\"Log FNLWGT: {}\".format(score_dataset_rf(train_X_logfnlwgt, val_X_logfnlwgt, train_y, val_y)))\n",
    "\n",
    "print(\"Normalized Features: {}\".\n",
    "      format(score_dataset_rf(train_X_num_normalized, val_X_num_normalized, train_y, val_y)))\n",
    "print(\"Standardized Features: {}\".\n",
    "      format(score_dataset_rf(train_X_num_standardized, val_X_num_standardized, train_y, val_y)))\n",
    "\n",
    "print(\"Log FNLWGT_std: {}\".format(score_dataset_rf(train_X_logfnlwgt_std, val_X_logfnlwgt_std, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests are robust to feature scaling and monotonic transformations because they split based on feature order, not magnitude.\n",
    "Thatâ€™s why all your versions of the dataset yield nearly identical results â€” the trees are effectively â€œseeingâ€ the same structure in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Engineering on Categorical Data\n",
    "\n",
    "In contrast to continuous numeric data we mean discrete values which belong to a specific finite set of categories or classes when we talk about categorical data. These discrete values can be text or numeric in nature and there are two major classes of categorical data, nominal and ordinal.\n",
    "\n",
    "While a lot of advancements have been made in state of the art machine learning frameworks to accept categorical data types like text labels. Typically any standard workflow in feature engineering involves some form of transformation of these categorical values into numeric labels and then applying some encoding scheme on these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "catCols = [cname for cname in train_X.columns if train_X[cname].dtype == \"object\"]\n",
    "\n",
    "train_X_cat = train_X[catCols].copy()\n",
    "val_X_cat = val_X[catCols].copy()\n",
    "\n",
    "simple_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "train_X_imputed = pd.DataFrame(simple_imputer.fit_transform(train_X_cat), columns=train_X_cat.columns, index=train_X_cat.index)\n",
    "val_X_imputed  = pd.DataFrame(simple_imputer.transform(val_X_cat), columns=val_X_cat.columns, index=val_X_cat.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One-Hot-Encoding\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GuntherGust/tds2_data/main/images/03/onehot.png\" style=\"width:70%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "Last week, we already talked about label and one-hot-encoding to prepare our categorical features for machine learning models. To get started, we will impute missing values and encode all categorical features using the ``OneHotEncoder``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "oh_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "train_X_OHenc = pd.DataFrame(oh_encoder.fit_transform(train_X_imputed), index=train_X_imputed.index)\n",
    "val_X_OHenc = pd.DataFrame(oh_encoder.transform(val_X_imputed), index=val_X_imputed.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again, we will use a helper function to evaluate the performance of our models. This time, we will rely on a logistic regression model (and ignore some convergence warning of the optimizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "def score_dataset_logreg(X_train, X_valid, y_train, y_valid):\n",
    "    model = LogisticRegression(max_iter=1000, random_state=0)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To evaluate the model we combine the raw numerical data and the encoded categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot encoded categorical + raw numeric: 0.8416656430413954\n"
     ]
    }
   ],
   "source": [
    "train_X_OH_num = train_X_num.join(train_X_OHenc.add_suffix(\"_OHenc\"))\n",
    "val_X_OH_num = val_X_num.join(val_X_OHenc.add_suffix(\"_OHenc\"))\n",
    "\n",
    "\n",
    "print(\"One-Hot encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset_logreg(train_X_OH_num, val_X_OH_num, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While label and one-hot encoding often yield good results, there are also a lot of other __(more complex) techniques to encode categorical variables.__ The package __[categorical-encoding](https://github.com/scikit-learn-contrib/categorical-encoding)__ offers implementations of many different techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Count Encodings\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GuntherGust/tds2_data/main/images/04/count-encoding.png\" style=\"width:70%\" />\n",
    "\n",
    "One prominent variant is called count encoding. Count encoding replaces each categorical value with the number of times it appears in the dataset. For example, if the value \"USA\" occures 50 times in the country feature, then each \"USA\" would be replaced with the number 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot encoded categorical + raw numeric: 0.7970765262252795\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import CountEncoder\n",
    "\n",
    "count_encoder = CountEncoder(handle_unknown=0, handle_missing='value')\n",
    "\n",
    "train_X_countenc = count_encoder.fit_transform(train_X_cat)\n",
    "val_X_countenc = count_encoder.transform(val_X_cat)\n",
    "\n",
    "train_X_count_num = train_X_num.join(train_X_countenc.add_suffix(\"_countenc\"))\n",
    "val_X_count_num = val_X_num.join(val_X_countenc.add_suffix(\"_countenc\"))\n",
    "\n",
    "print(\"One-Hot encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset_logreg(train_X_count_num, val_X_count_num, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Target Encodings\n",
    "\n",
    "Target encoding is another advanced (but sometimes dangerous) approach to encode categorical features. It replaces a categorical value with the average value of the target for that value of the feature. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/GuntherGust/tds2_data/main/images/04/target-encoding.png\" style=\"width:70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For example, given the country value \"GER\", you'd calculate the average outcome for all the rows with country == 'GER'. This value is often blended with the target probability over the entire dataset to reduce the variance of values with few occurences and to avoid overfitting. See [here](https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69) for a more detailed explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Attention:__ This technique uses the targets to create new features. So including the validation or test data in the target encodings would be a form of target leakage. Instead, you should learn the target encodings from the __training dataset only__ and apply it to the other datasets (as we did with all other encoding methods).\n",
    "\n",
    "See also [here](https://medium.com/analytics-vidhya/target-encoding-vs-one-hot-encoding-with-simple-examples-276a7e7b3e64) for another illustrative example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoded categorical + raw numeric: 0.8449821889202801\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "target_encoder = TargetEncoder()\n",
    "\n",
    "train_X_targetenc = target_encoder.fit_transform(train_X_cat, train_y)\n",
    "val_X_targetenc = target_encoder.transform(val_X_cat)\n",
    "\n",
    "train_X_target_num = train_X_num.join(train_X_targetenc.add_suffix(\"_targetenc\"))\n",
    "val_X_target_num = val_X_num.join(val_X_targetenc.add_suffix(\"_targetenc\"))\n",
    "\n",
    "print(\"Target encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset_logreg(train_X_target_num, val_X_target_num, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CatBoost Encoding\n",
    "\n",
    "Finally, we'll look at CatBoost encoding. CatBoost extends the target encoding approach. It reduces problems of target encoding regarding target leakage and overfitting.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/GuntherGust/tds2_data/main/images/04/catboost-encoding.png\" style=\"width:50%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Roughtly, Catboost encoding is similar to target encoding in that it's based on the target probablity for a given value. However with CatBoost, for each row, the __target probability__ is calculated only from __the rows before it__. Interested readers may find more details [here](https://towardsdatascience.com/how-catboost-encodes-categorical-variables-3866fb2ae640). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost encoded categorical + raw numeric: 0.8449821889202801\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import CatBoostEncoder\n",
    "\n",
    "catboost_encoder = CatBoostEncoder()\n",
    "\n",
    "train_X_catboostenc = catboost_encoder.fit_transform(train_X_cat, train_y)\n",
    "val_X_catboostenc = catboost_encoder.transform(val_X_cat)\n",
    "\n",
    "train_X_catboost_num = train_X_num.join(train_X_catboostenc.add_suffix(\"_targetenc\"))\n",
    "val_X_catboost_num = val_X_num.join(val_X_catboostenc.add_suffix(\"_targetenc\"))\n",
    "\n",
    "print(\"CatBoost encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset_logreg(train_X_catboost_num, val_X_catboost_num, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Summary: Comparison of encoding performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot encoded categorical + raw numeric: 0.8416656430413954\n",
      "Count encoded categorical + raw numeric: 0.7970765262252795\n",
      "Target encoded categorical + raw numeric: 0.8449821889202801\n",
      "CatBoost encoded categorical + raw numeric: 0.8449821889202801\n"
     ]
    }
   ],
   "source": [
    "print(\"One-Hot encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset_logreg(train_X_OH_num, val_X_OH_num, train_y, val_y)))\n",
    "print(\"Count encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset_logreg(train_X_count_num, val_X_count_num, train_y, val_y)))\n",
    "print(\"Target encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset_logreg(train_X_target_num, val_X_target_num, train_y, val_y)))\n",
    "print(\"CatBoost encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset_logreg(train_X_catboost_num, val_X_catboost_num, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Warning\n",
    "\n",
    "Target encoding is a powerful but dangerous way to improve on your machine learning methods. \n",
    "\n",
    "Advantages: \n",
    "* Compact transformation of categorical variables\n",
    "* Powerful basis for feature engineering\n",
    "\n",
    "Disadvantages:\n",
    "* Careful validation is required to avoid overfitting\n",
    "* Significant performance improvements only on some datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercises\n",
    "\n",
    "1. What theoretical issues do you encounter when combining the logistic regression with the different categorical encoding techniques? \n",
    "\n",
    "2. Evaluate the categorical encoding techniques using your `score_dataset_rf` function from the previous exercise. In theory, tree-based algorithms, such as random forests, do not require specific encodings. Why do you still observe some differences in the results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Target and CatBoost encoders** convert nominal categories into **continuous numeric scales**, implicitly creating an **ordering and distance structure** between categories that does **not exist in reality**. Logistic regression, being a **linear model**, then **treats that artificial order as meaningful**, which can **distort the relationship** between features and the target â€” even if predictive accuracy remains good.\n",
    "\n",
    "\n",
    "**Count encoding** similarly introduces a **pseudo-order** because more frequent categories are assigned **larger numeric values**.  \n",
    "Logistic regression interprets this as a **linear trend in frequency**, even though frequency itself may not be **causally or semantically related** to the target, again imposing an **unintended ordinal structure**.\n",
    "\n",
    "**One-hot encoding**, in contrast, preserves the **pure nominal nature** of categories â€” each becomes its own **independent binary feature**, with **no implied order or distance**.  This keeps the representation **theoretically sound** for logistic regression, at the cost of **higher dimensionality** and potential **multicollinearity** (which can be handled by dropping one dummy or applying regularization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot encoded categorical + raw numeric: 0.8497727551897801\n",
      "Count encoded categorical + raw numeric: 0.8522294558408058\n",
      "Target encoded categorical + raw numeric: 0.8534578061663186\n",
      "CatBoost encoded categorical + raw numeric: 0.8630389387053188\n"
     ]
    }
   ],
   "source": [
    "### Your code here....\n",
    "print(\"One-Hot encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset_rf(train_X_OH_num, val_X_OH_num, train_y, val_y)))\n",
    "print(\"Count encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset_rf(train_X_count_num, val_X_count_num, train_y, val_y)))\n",
    "print(\"Target encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset_rf(train_X_target_num, val_X_target_num, train_y, val_y)))\n",
    "print(\"CatBoost encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset_rf(train_X_catboost_num, val_X_catboost_num, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, Random Forests are encoding-agnostic because they split on thresholds, not distances. Despite the theory, youâ€™ll typically observe slight (sometimes large) differences in accuracy or feature importance between encoders when using Random Forests:\n",
    "\n",
    "- Different encodings change the feature space geometry. One-hot encoding expands each categorical variable into multiple binary features. Trees can now split independently on each category, allowing very fine-grained partitions. Ordinal or count encoding compresses categories into a single numeric variable. The tree can only split on numeric thresholds â€” which imposes an artificial order and limits flexibility. Even though the algorithm can handle either, its split options differ, affecting the learned structure.\n",
    "\n",
    "- Random Forests use bootstrapping (random sampling of rows), and randomly select a subset of features at each split (max_features). If encodings change the number of features (as in one-hot encoding), then the probability of each original variable being selected changes. The importance and contribution of features differ slightly â€” leading to different overall models and scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Today, we have seen a variety of ways to encode numerical and categorical features to improve the performance of our machine learning models. To try even more encoding methods you can try the implementations in the categorical-encoding package on [github](https://github.com/scikit-learn-contrib/categorical-encoding).\n",
    "\n",
    "While the approaches we have talked about today have the potential to create powerful models, they require a lot of manual tuning and testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Mentimeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/d3.png\" style=\"width:80%; float:center;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (PDS_04)",
   "language": "python",
   "name": "pds_04"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "rise": {
   "enable_chalkboard": false,
   "overlay": "<div class='background'></div><div class='header'></br>PDS</div><div class='logo'><img src='images/d3logo.png'></div><div class='bar'></div>",
   "scroll": true,
   "slideNumber": "h.v"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
