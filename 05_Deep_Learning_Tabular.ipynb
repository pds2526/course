{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/tds2023-24/course/blob/main/notebooks/05_Deep_Learning_Tabular.ipynb\n",
    "\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class='bar_title'></div>\n",
    "\n",
    "*Practical Data Science*\n",
    "\n",
    "# Deep Learning on Tabular Data\n",
    "\n",
    "Gunther Gust<br>\n",
    "Chair of Enterprise AI <br>\n",
    "Data Driven Decisions Group &<br>\n",
    "Center for Artificial Intelligence and Data Science (CAIDAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/d3.png\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/CAIDASlogo.png\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Credits__\n",
    "\n",
    "<img src=\"https://images-na.ssl-images-amazon.com/images/I/516YvsJCS9L._SX379_BO1,204,203,200_.jpg\" width=\"200\" align=\"right\"/>\n",
    "\n",
    "In the next lectures we will dive into Deep Learning using ressources from the book of \n",
    "**Jeremy Howard and Sylvian Gugger: \"Deep Learning for Coders with Fastai and PyTorch: AI Applications without a PhD.\" (2020).**\n",
    "\n",
    "It's freely available as interactive [Jupyter Notebook](https://github.com/fastai/fastbook) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Materials also taken from:\n",
    "- https://github.com/fastai/fastbook/blob/master/09_tabular.ipynb\n",
    "- https://www.fast.ai/2018/04/29/categorical-embeddings/\n",
    "- https://confusedcoders.com/data-science/deep-learning/how-to-apply-deep-learning-on-tabular-data-with-fastai "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of 2015, the [Rossmann sales competition](https://www.kaggle.com/c/rossmann-store-sales) ran on Kaggle. Competitors were given a wide range of information about various stores in Germany, and were tasked with trying to predict sales on a number of days. The goal was to help the company to __manage stock__ properly and be able to __satisfy demand without holding unnecessary inventory.__ The official training set provided a lot of information about the stores. It was also permitted for competitors to use additional data, as long as that data was made public and available to all participants.\n",
    "\n",
    "One of the gold medalists used deep learning, in one of the __earliest known examples__ of a __state-of-the-art deep learning tabular model.__ Their method involved far __less feature engineering__, based on domain knowledge, than those of the other gold medalists. The paper, [\"Entity Embeddings of Categorical Variables\"](https://arxiv.org/abs/1604.06737) describes their approach. The authors state:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Entity embedding not only **reduces memory usage** and **speeds up neural networks compared with one-hot encoding**, but more importantly by **mapping similar values close to each other in the embedding space** it reveals the intrinsic properties of the categorical variables... \n",
    "\n",
    "> [It] is especially useful for datasets with lots of **high cardinality features**, where other methods tend to overfit... As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Categorical Embedding Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a look at the examples from the paper [\"Entity Embeddings of Categorical Variables\"](https://arxiv.org/abs/1604.06737) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**State embeddings and map**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"State embeddings and map\" width=\"50%\" caption=\"State embeddings and map (courtesy of Cheng Guo and Felix Berkhahn)\" id=\"state_emb\" src=\"https://raw.githubusercontent.com/fastai/fastbook/master/images/att_00015.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the left is a plot of the embedding matrix for the possible values of the `State` category. For a categorical variable we call the possible values of the variable its \"levels\" (or \"categories\" or \"classes\"), so here one level is \"Berlin,\" another is \"Hamburg,\" etc. On the right is a map of Germany. The actual physical locations of the German states were not part of the provided data, yet the model itself learned where they must be, based only on the behavior of store sales!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Store Distances**\n",
    "\n",
    "The distance between store embeddings against the actual geographic distance between the stores - they match very closely!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Store distances\" width=\"50%\" caption=\"Store distances (courtesy of Cheng Guo and Felix Berkhahn)\" id=\"store_emb\" src=\"https://raw.githubusercontent.com/fastai/fastbook/master/images/att_00016.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Date Embedding**\n",
    "\n",
    "Days and months that are near each other on the calendar ended up close as embeddings too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Date embeddings\" width=\"50%\" caption=\"Date embeddings\" id=\"date_emb\" src=\"https://raw.githubusercontent.com/fastai/fastbook/master/images/att_00017.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we train such embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__What are neural networks?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "- Biological neural networks have interconnected neurons with dendrites that receive inputs, then based on these inputs they produce an output signal through an axon to another neuron\n",
    "- Artificial Neural Networks (ANN) are a machine learning framework that attempts to mimic the learning pattern of natural biological neural networks\n",
    "- The creation of ANN begins with the most basic form, a single perceptron\n",
    "\n",
    "<img src=\"./images/05/DALL·E 2023-10-07 15.23.07.png\" width=\"30%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "Developed by Frank Rosenblatt in 1957\n",
    "- Perceptrons have one or more weighted inputs, a bias, an activation function, and a single output\n",
    "- A perceptron receives inputs, multiplies them by some weight, and then passes them into an activation function to produce an output\n",
    "- The key idea is to “fire” / activate the neuron only if a sufficiently strong input signal is detected\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2870/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" width=\"50%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Different Activation Functions and their Graphs__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png\" width=\"50%\"/>\n",
    "\n",
    "[Image Source](https://medium.com/@shrutijadon10104776/survey-on-activation-functions-for-deep-learning-9689331ba092)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "ReLU is widely used as an activation function because its derivative is easy to compute during backpropagation, making it both simple and computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multi-layer Perceptron aka. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MLP is composed of multiple layers of perceptrons \n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/8a0c06dc8a564e1e7732169d97e2685eafc2b98b5f4915f4c0cb6bf2c1649a37/68747470733a2f2f7777772e64726f70626f782e636f6d2f732f717334746f6a763575356834386c662f6d756c74696c617965725f70657263657074726f6e2e706e673f7261773d31\" style=\"width:80%\" />\n",
    "\n",
    "[Image Source](https://github.com/PetarV-/TikZ/tree/master/Multilayer%20perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Layers of a MLP__\n",
    "\n",
    "- Initial layer = input layer which is fed by the feature inputs\n",
    "- Last layer = output layer which creates the resulting outputs\n",
    "- Any layers in between are known as hidden layers because they do not directly “observe” the feature inputs or outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Universal approximation theorem__\n",
    "\n",
    "From Wikipedia:\n",
    "\n",
    "_\"In the mathematical theory of artificial neural networks, the universal approximation theorem states that a feed-forward network with __a single hidden layer__ containing a finite number of neurons can approximate any continuous function [...] when given appropriate parameters; however, it does not touch upon __the algorithmic learnability of those parameters__.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Loss function__\n",
    "\n",
    "The loss function $L$ is a way for a model to measure how well it’s doing. Imagine you have a goal, like correctly classifying pictures of cats and dogs. Each time the model makes a guess, the loss function calculates how far off that guess is from the true answer. It gives a number (called \"loss\" or \"error\") that __tells you how bad the guess was.__\n",
    "\n",
    "The goal of __training__ the model is to make the loss as small as possible by __minimizing the loss function.__\n",
    "\n",
    "For more explanation see e.g. [this video](https://youtu.be/QBbC3Cjsnjg). Common choices are __MeanSquaredError (MSE)__ loss for regression tasks and __CrossEntropy loss__ for classification tasks, see [here](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html) for the math. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Training Neural Networks\n",
    "\n",
    "Learning is adjustment of the weights of the connections between perceptrons according to some modification rule. \n",
    "\n",
    "- The Backpropagation algorithm searches for weight values that minimize the total error of the network over the set of training examples\n",
    "\n",
    "It consists of the repeated application of the following two passes.\n",
    "\n",
    "- __Forward pass__: in this step the network is activated on one example and the error of (each neuron of) the output layer is computed\n",
    "- __Backward pass__: in this step the network error is used for updating the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Forward and Backward passes__\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/3108/1*6q2Rgd8W9DoCN9Wfwc_9gw.png\" style=\"width:60%\" />\n",
    "\n",
    "[Image Source](https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Core Ideas of Backpropagation\n",
    "\n",
    "### 1. Compute the gradient for each parameter\n",
    "\n",
    "Backpropagation is built on the idea that we can compute __how the loss $L$ changes__ when we change a single __weight__ $w$. This is captured by the partial derivative\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "which tells us whether increasing $w$ makes the loss larger or smaller, and by how much. Backpropagation efficiently computes these gradients for __*all* parameters__ by applying the __chain rule__ through the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Update each parameter using the gradient\n",
    "\n",
    "Once the gradient is known, each weight is updated using gradient descent:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "This moves the parameter in the direction that reduces the loss. Small steps are taken repeatedly until the network learns to produce better predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For an illustrative explanation of how to train a neural network using backpropagation see [this](https://www.youtube.com/watch?v=iyn2zdALii8&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=6&ab_channel=StatQuestwithJoshStarmer) and related videos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing a Multi Layer Perceptron\n",
    "\n",
    "We will work with the same dataset as in the last lecture, a sample of the adult dataset which has some census information on individuals. Again, we'll use it to train a model to predict whether salary is greater than $50k or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'https://raw.githubusercontent.com/GuntherGust/tds2_data/main/data/adult.csv'\n",
    "adult_data = pd.read_csv(file_path)\n",
    "adult_data = adult_data.assign(salary=(adult_data['salary']=='>=50k').astype(int))\n",
    "y = adult_data['salary']\n",
    "X = adult_data.drop(columns=['salary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Split data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Impute missing values (we will omit the categorical features here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer = SimpleImputer()\n",
    "numCols = X.select_dtypes(['int', 'float']).columns.to_list()\n",
    "train_X_num = pd.DataFrame(simple_imputer.fit_transform(train_X[numCols]), columns=numCols, index=train_X.index)\n",
    "val_X_num = pd.DataFrame(simple_imputer.transform(val_X[numCols]), columns=numCols, index=val_X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Standardize numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_X_num_standardized = pd.DataFrame(scaler.fit_transform(train_X_num), columns=numCols, index=train_X.index)\n",
    "val_X_num_standardized = pd.DataFrame(scaler.transform(val_X_num), columns=numCols, index=val_X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()\n",
    "model.fit(train_X_num_standardized, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```MLPClassifier()``` allows to create arbitrarily-sized neural networks. For example:\n",
    "\n",
    "- ```MLPClassifier(hidden_layer_sizes=())``` A simple perceptron (no hidden layer)\n",
    "- ```MLPClassifier(hidden_layer_sizes=(10,))``` Creates a NN with one hidden layer with 10 neurons\n",
    "- ```MLPClassifier()``` Creates a NN with one hidden layer with 100 neurons (default setting)\n",
    "- ```MLPClassifier(hidden_layer_sizes=(128, 64, 32, 16))``` A deeper 4-layer network: Input → 128 → 64 → 32 → 16 → Output\n",
    "\n",
    "Many __more important parameters,__ such as _activation functions_, _optimizer_, etc., can be configured. Check out the [official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(val_X_num_standardized)\n",
    "accuracy_score(val_y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Advantages of Multi-layer Perceptrons__\n",
    "\n",
    "- Capability to learn non-linear models.\n",
    "- Capability to learn models in real-time (on-line learning) using `partial_fit`\n",
    "\n",
    "__The disadvantages of Multi-layer Perceptrons__\n",
    "- MLP with hidden layers have a non-convex loss function where there exists more than one __local minimum.__ Therefore different random weight initializations can lead to different validation accuracy.\n",
    "- MLP requires tuning a number of __hyperparameters__ such as the number of hidden neurons, layers, and iterations.\n",
    "- MLP is sensitive to __feature scaling.__\n",
    "\n",
    "[from scikit-learn](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Is this already deep learning?__\n",
    "\n",
    "From Wikipedia: \n",
    "\n",
    "_\"Deep learning [...] uses multiple layers to progressively extract higher level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.\"_ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercises - XOR Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The XOR (exclusive OR) is a logical operation that takes two binary outputs and returns TRUE if exactly one of them is TRUE. For example:\n",
    "- f(1, 0) = 1\n",
    "- f(1, 1) = 0\n",
    "- f(0, 1) = 1\n",
    "- f(0, 0) = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "___Background:___ The XOR problem has a famous history with regard to neural networks:\n",
    "\n",
    "- XOR is not linearly separable, so a single-layer perceptron cannot learn it.\n",
    "- This limitation, highlighted by Minsky & Papert (1969), led to broad skepticism about neural networks.\n",
    "- Adding a hidden layer allows a neural network to solve XOR, showing the power of multilayer models.\n",
    "- The success of backpropagation in training such multilayer networks helped spark the modern deep learning era."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We provide you a XOR-like dataset with 80 samples and 2 labels as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN & DO NOT MODIFY\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create XOR dataset\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create points around the corners of the XOR problem\n",
    "X_corners = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_corners = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Generate additional random samples in each quadrant\n",
    "num_samples_per_corner = 20\n",
    "X_samples = []\n",
    "y_samples = []\n",
    "\n",
    "for x, y in X_corners:\n",
    "    # Generate samples around each corner\n",
    "    X_samples.append(np.random.rand(num_samples_per_corner, 2) * 0.2 + np.array([x - 0.1, y - 0.1]))\n",
    "    y_samples.append(np.full((num_samples_per_corner,), y_corners[np.where((X_corners == [x, y]).all(axis=1))[0][0]]))\n",
    "\n",
    "X_samples = np.vstack(X_samples)\n",
    "y_samples = np.concatenate(y_samples)\n",
    "\n",
    "# Step 2: Split Data into Training and Validation Sets\n",
    "# Ensure that each class is represented in both sets\n",
    "indices = np.arange(len(X_samples))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Manually split to maintain distribution\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for i in range(4):  # For each corner\n",
    "    corner_indices = indices[i*num_samples_per_corner:(i+1)*num_samples_per_corner]\n",
    "    train_indices.extend(corner_indices[:int(num_samples_per_corner * 0.7)])  # 70% for training\n",
    "    val_indices.extend(corner_indices[int(num_samples_per_corner * 0.7):])    # 30% for validation\n",
    "\n",
    "train_X_xor = X_samples[train_indices]\n",
    "train_y_xor = y_samples[train_indices]\n",
    "val_X_xor = X_samples[val_indices]\n",
    "val_y_xor = y_samples[val_indices]\n",
    "\n",
    "plt.scatter(X_samples[y_samples == 0][:, 0], X_samples[y_samples == 0][:, 1], color='red', label='Class 0')\n",
    "plt.scatter(X_samples[y_samples == 1][:, 0], X_samples[y_samples == 1][:, 1], color='blue', label='Class 1')\n",
    "plt.title('XOR Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your tasks are:\n",
    "\n",
    "1. Define an MLP with one hidden layer with one neuron and fit this dataset. Run 1000 iteration.\n",
    "2. Define an MLP with one hidden layers with eight neurons and fit this dataset. Run 1000 iteration.\n",
    "3. Look at the decision boundary, and try to explain the results. Hint: check which model provides better decision boundary, and why it is the case.\n",
    "\n",
    "Hint: use `train_X_xor, train_y_xor` for training, and `val_X_xor`, `val_y_xor` for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# define, fit, and predict the first model with one hidden layer. \n",
    "# Hint 1: MLPClassifier(hidden_layer_sizes=(number_layer, ), max_iter=num_iterations)\n",
    "# Hint 2: the accuracy should never reach 80% \n",
    "mlp_hidden_1 = ... # define the model\n",
    "mlp_hidden_1.fit(...) # fit the model\n",
    "preds_hidden_1 = ... # predict the model\n",
    "accuracy_score(...) # compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# define, fit, and predict the first model with 8 hidden layers. \n",
    "# Hint 1: MLPClassifier(hidden_layer_sizes=(number_layer, ), max_iter=num_iterations)\n",
    "# Hint 2: the accuracy can reach 100%. Try to re-run until you get accuracy=1.0\n",
    "mlp_hidden_8 = ... # define the model\n",
    "mlp_hidden_8.fit(...) # fit the model\n",
    "preds_hidden_8 = ... # predict the model\n",
    "accuracy_score(...) # compute the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the decision boundary of 2 cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN & DO NOT MODIFY\n",
    "def plot_decision_boundary(model, val_X, val_y, num_layer):\n",
    "    xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])  # Predicting over the grid\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plotting decision boundary and validation samples\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "\n",
    "    # Plot validation points\n",
    "    plt.scatter(val_X[val_y == 0][:, 0], val_X[val_y == 0][:, 1], color='darkred', label='Val Class 0', marker='x')\n",
    "    plt.scatter(val_X[val_y == 1][:, 0], val_X[val_y == 1][:, 1], color='darkblue', label='Val Class 1', marker='x')\n",
    "\n",
    "    plt.title(f'Decision Boundary of MLP with {num_layer} Hidden Layer')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary of MLP with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this after finishing the above tasks\n",
    "plot_decision_boundary(mlp_hidden_1, val_X_xor, val_y_xor, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary of MLP with 8 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this after finishing the above tasks\n",
    "plot_decision_boundary(mlp_hidden_8, val_X_xor, val_y_xor, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning on Tabular Data with *fast.ai*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**The Mission of [fast.ai](https://www.fast.ai/about.html): Making neural nets uncool again**\n",
    "\n",
    "Deep learning is transforming the world. We are making deep learning easier to use and getting more people from all backgrounds involved through our:\n",
    "\n",
    "- [free courses for coders](http://course.fast.ai/)\n",
    "- software library: [fastai for PyTorch](http://docs.fast.ai/)\n",
    "- cutting-edge research\n",
    "- community\n",
    "\n",
    "The world needs everyone involved with AI, no matter how unlikely your background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, let's import everything we need for the tabular application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -Uqq fastai  # upgrade fastai on colab\n",
    "from fastai.tabular.all import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from <module> import *` means “I want access to all the names in <module> that I’m meant to have access to”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### *fast.ai* Datasets\n",
    "\n",
    "Tabular data usually comes in the form of a delimited file (such as .csv) containing variables of different kinds: text/category, numbers, and perhaps some missing values. \n",
    "\n",
    "*Fast.ai's* [external data functions](https://docs.fast.ai/data.external.html) provides several useful datasets that we might be interested in using in our models.\n",
    "\n",
    "We will work with the same dataset as in the last lecture, a sample of the __adult dataset__ which has some census information on individuals. Again, we'll use it to train a model to predict whether __salary is greater than \\$50k__ or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(url=URLs.ADULT_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`untar_data()`downloads a dataset from `url` and unpacks it to `path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'adult.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Here all the information that will form our input is in the 14 first columns, and the dependent variable is the last column. We will split our input between two types of variables: categorical and continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### From data to dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai uses [data loaders](https://docs.fast.ai/data.load.html) to get the data ready for training.\n",
    "\n",
    "A data loader usually combines a __dataset and a sampler,__ and provides an __iterable__ over the given dataset. [fastai](https://docs.fast.ai/data.load.html) includes a replacement for [Pytorch's DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) which is largely API-compatible, and adds a lot of useful functionality and flexibility.\n",
    "\n",
    "How do we create a data loader?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Categorical and continuous variables__\n",
    "\n",
    "- **Categorical variables** (like workclass or education) will be replaced by a category - a unique id that identifies them - before they are passed through an embedding layer.\n",
    "- **Continuous variables** (like age) will be normalized and then directly fed to the model.\n",
    "\n",
    "We can specify our categorical and continuous column names, as well as the name of the dependent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_names = 'salary'\n",
    "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "cont_names = ['age', 'fnlwgt', 'education-num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Tabular data preprocessing__\n",
    "\n",
    "fast.ai contains classes that define [transformations](https://docs.fast.ai/tabular.core.html#TabularProc) for preprocessing dataframes of tabular data. Preprocessing includes things like\n",
    "- `FillMissing`: filling missing values for continuous variables (default fill strategy: median). Missing values in categorical columns are handled automatically by `Categorify`, which treats ```NaN, None, ''``` as a separate category.\n",
    "- `Categorify`: replacing non-numerical variables by categories (each value obtains a category id, as in label encoding). \n",
    "- `Normalize:` normalizing continuous variables. (Categorical columns are __not__ normalized. Category indices are passed directly into an embedding layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can define a list of Transforms that will be applied to our variables. Here we transform all categorical variables into categories. We also replace missing values for continuous variables by the median column value and normalize those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procs = [FillMissing, Categorify, Normalize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that the __ordering__ of the tranformations matters. The typical sequence is to first handle missing values, then categorize categorical variables (including newly created categorial variables that indicate missingness), and finally normalize continuous variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Training and validation sets__\n",
    "\n",
    "To split our data into training and validation sets, we use valid indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, valid_idx = train_test_split(df.index, test_size=0.25, random_state = 0)\n",
    "train_idx[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Creating the DataLoader__\n",
    "\n",
    "Now we're ready to pass this information to a [TabularDataLoader](https://docs.fast.ai/tabular.data.html#TabularDataLoaders) to create the DataLoaders that we'll use for training. We will learn the details of `DataLoaders` class in the next lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, \n",
    "                                  y_names=y_names,\n",
    "                                  cat_names = cat_names,\n",
    "                                  cont_names = cont_names,\n",
    "                                  valid_idx=valid_idx,\n",
    "                                  procs = procs,\n",
    "                                 bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.cat_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can grab a mini-batch of data and take a look. `show_batch` shows a batch of data in a convenient way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: As we pick out batches randomly, the output of `show_batch` may not correspond to the output below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After being processed, the categorical variables are replaced by ids and the continuous variables are normalized. The codes corresponding to categorical variables are all put together, as are all the continuous variables.\n",
    "\n",
    "But how does the data exactly look like for our model? Let's have a look:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = dls.one_batch()\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Question__: Can you explain the individual components of the previous outputs?\n",
    "- I.e. what is contained in the individual tensors? (Hint: look at the dimensions of the tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once we have our data ready in `DataLoaders`, we just need to create a model to then define a Learner and start training. \n",
    "\n",
    "This is typically composed of following steps :\n",
    "\n",
    "1. __Create Learner__: Create an appropriate learner for data. A learner creates a neural network for us.\n",
    "2. __Find the learning rate__: We need to find a suitable learning rate for our training\n",
    "3. __Fit the model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Create Learner__\n",
    "\n",
    "The fastai library has a flexible and powerful `TabularModel`. The `tabular_learner` will automatically create a `TabularModel` suitable for your data and infer the right loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(dls, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's print a summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualizing the model graph with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables \n",
    "\n",
    "- tracking experiment metrics like loss and accuracy, \n",
    "- visualizing the model graph, \n",
    "- projecting embeddings to a lower dimensional space, \n",
    "- and much more.\n",
    "\n",
    "Let's load the TensorBoard notebook extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The SummaryWriter class is your main entry to log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create a writer (for later logging the model and visualizing it in tensorboard) in the directory \"tb-tabular\"\n",
    "writer = SummaryWriter('tb-tabular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Write model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch one batch of data\n",
    "batch = dls.one_batch() #alternatively: next(iter(dls.train))\n",
    "# load the model to the cpu and pass some instances of the batch through it (this is necessary for the writer to log the computational graph)\n",
    "writer.add_graph(learn.model.cpu(), batch[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Start TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start tensorboard providing the previously specified directory\n",
    "%tensorboard --logdir tb-tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of displaying tensorboard in the jupyter notebook cell, it may be better to open it in a new browser tab using \"localhost:<port>\", e.g \"localhost:6006\" in the address bar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For an additional example on how to analyze model architectures with tensorboard, see e.g. [this video](https://youtu.be/9SdLOcGnebU?t=567). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train the model\n",
    "\n",
    "__Find the learning rate__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate finder will do a __mock training__ by going over a large range of learning rates, then __plot them against the losses.__ We typically find the point where the slope is steepest. \n",
    "\n",
    "We will learn more about the Learning Rate finder and 1cycle policy, but here are some excellent resources that offer explanation: \n",
    "* https://fastai1.fast.ai/callbacks.one_cycle.html\n",
    "* https://sgugger.github.io/the-1cycle-policy.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Fit the model__ based on selected learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, lr_max=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then have a look at some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Get predictions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can use the `Learner.predict` method to get predictions. In this case, we need to pass the row of a dataframe that has the same names of categorical and continuous variables as our training or validation dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row, clss, probs = learn.predict(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clss, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Calculate performance metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... shows the validation loss and the validation metric (accuracy). \n",
    "We can manually compute this as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, val_y = learn.get_preds(ds_idx=1)\n",
    "preds = np.argmax(probs, axis=1)\n",
    "accuracy_score(val_y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To get prediction on a new dataframe, you can use the `test_dl` method of the DataLoaders. That dataframe does not need to have the dependent variable in its column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.copy()\n",
    "test_df.drop(['salary'], axis=1, inplace=True)\n",
    "dl = learn.dls.test_dl(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Show rows result of predictions on the dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.get_preds(dl=dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is a scope of improving the deep learning model here. However this is not bad at all, without any feature engineering and network tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Embeddings for Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A key technique to making the most of deep learning for tabular data is to use embeddings for your categorical variables. This approach allows for __relationships between categories__ to be captured.\n",
    "\n",
    "Examples:\n",
    "- Saturday and Sunday may have similar behavior, and maybe Friday behaves like an average of a weekend and a weekday. \n",
    "-  Similarly, for zip codes, there may be patterns for zip codes that are geographically near each other, and for zip codes that are of similar socio-economic status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applying Embeddings for Categorical Variables\n",
    "\n",
    "When working with categorical variables, we will represent each category by a vector of floating point numbers (the values of this representation are learned as the network is trained).\n",
    "\n",
    "For instance, a 4-dimensional version of an embedding for day of week could look like:\n",
    "\n",
    "__Sunday\t [.8, .2, .1, .1]__<br>\n",
    "__Monday\t[.1, .2, .9, .9]__<br>\n",
    "__Tuesday\t[.2, .1, .9, .8]__\n",
    "\n",
    "Here, Monday and Tuesday are fairly similar, yet they are both quite different from Sunday. \n",
    "\n",
    "Again, this is a toy example. In practice, our neural network would learn the best representations for each category while it is training, and each dimension (or direction, which doesn’t necessarily line up with ordinal dimensions) could have multiple meanings. Rich relationships can be captured in these distributed representations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The choice of the dimensions of the embeddings depends on many factors, such as training set size, complexity of the learning task and computational resources available.\n",
    "\n",
    "Fast.ai (see [emb_sz_rule](https://docs.fast.ai/tabular.model.html#emb_sz_rule)) applies the following rule of thumb: \n",
    "- __Embedding size = min(50, round(1.6 * n_categories^0.56))__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Visualizing Embeddings with Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Export embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, emb in enumerate(learn.model.embeds):\n",
    "    emb_name = learn.dls.cat_names[i]\n",
    "    writer.add_embedding(emb.weight.data, metadata=learn.dls.classes[emb_name],\n",
    "                         global_step=i, tag=emb_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Finally, start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir tb-tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___Colab Workaround___\n",
    "\n",
    "In Colab the dynamic tensorboard plugin isn’t supported yet, but you can still access the data and visualize the embeddings somewhere else: \n",
    "\n",
    "1. Download the desired embedding file (*tensors.tsv*) and metadata \n",
    "2. Upload the files on the official Tensorflow [Embedding Projector](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the [webpage](https://projector.tensorflow.org/) of the Embedding Projector, you can also explore example word and image embeddings. Feel free to try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentimeter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "For analyzing time series and tabular data, deep learning has recently been making great strides. However, deep learning is generally used as part of an **ensemble of multiple types** of model. \n",
    "\n",
    "- If you already have a system that is using random forests or gradient boosting machines, then switching to or adding deep learning may not result in any dramatic improvement. \n",
    "- Deep learning does greatly increase the variety of columns that you can include\n",
    "    - columns containing natural language (book titles, reviews, etc.), \n",
    "    - high-cardinality categorical columns (i.e., something that contains a large number of discrete choices, such as zip code or product ID). \n",
    "- Deep learning models generally take longer to train than random forests or gradient boosting machines, although this is changing thanks to libraries such as [RAPIDS](https://rapids.ai/), which provides GPU acceleration for the whole modeling pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final Notes\n",
    "\n",
    "* The material of this lecture is very advanced if you are not familiar to neural networks\n",
    "* You may be able to complete the course without diving deeper into them\n",
    "* However, to get most out of this class (and prepare yourself to become a data scientists), we highly recommend to check out the additional material provided. In particular:\n",
    "    * the materials and courses offeerd by [fastai](https://www.fast.ai/about.html)\n",
    "    * The book: Jeremy Howard and Sylvian Gugger (2020), \"Deep Learning for Coders with Fastai and PyTorch: AI Applications without a PhD.\" (2020). It's freely available as interactive [Jupyter Notebook](https://github.com/fastai/fastbook) \n",
    "    * Also Andrew NGs [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning?utm_medium=sem&utm_source=gg&utm_campaign=B2C_EMEA_deep-learning_deeplearning-ai_FTCOF_specializations_country-DE&campaignid=20416373453&adgroupid=155810822830&device=c&keyword=deep%20learning%20specialization&matchtype=b&network=g&devicemodel=&adposition=&creativeid=667829385242&hide_mobile_promo&gclid=Cj0KCQiAr8eqBhD3ARIsAIe-buNiwv5lVHgaI7bjlWFq52LxRgQNdlvNCevnV_33f_ZX8Dc4wKwLIUEaAiTCEALw_wcB) is a great course for learning more about the theory behind neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/d3.png\" style=\"width:50%; float:center;\" />"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "rise": {
   "enable_chalkboard": false,
   "overlay": "<div class='background'></div><div class='header'></br>PDS</div><div class='logo'><img src='images/d3logo.png'></div><div class='bar'></div>",
   "scroll": true,
   "slideNumber": "h.v"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
